---
title: "Introduction to Stacking Models with `{stacks}`"
format: html
theme: flatly
---

```{r global-options, include=FALSE}
#knitr::opts_chunk$set(eval = FALSE)
library(tidyverse)
library(tidymodels)
library(patchwork)
library(kableExtra)
```

**Purpose:** In this notebook we'll continue our exploration of *ensembles* by looking customizable model stacks.

## The Big Idea

We've spent a few class meetings talking about ensembles of models. Our previous two topics focused on *random forests* and *gradient boosted trees*, two very common types of ensemble with implementations in `{tidymodels}`. It is possible, however, to use custom ensembles. 

Say, for example, we fit several classifiers -- a logistic regression model, nearest neighbors, support vector machines, and decision trees -- perhaps each with different hyperparameters. We may not simply want to choose the model that performs best from our collection -- perhaps we can achieve better results if we *blend* the predictions from each of our models together. This is the purpose of the `{stacks}` package.

## How It Works

The strategy for fitting a stack of models is fairly straight-forward.

+ Determine a collection of candidate models for potential inclusion in your stack.
+ Define the model specifications, recipes, and workflows for those candidate models and tune/fit them using cross-validation.
+ Initialize a model *stack* with the `stack()` function and add model candidates to it by piping to `add_candidates()` with the tuned models as arguments. 
+ Fit the stack using `blend_predictions()`.
+ Use the `fit_members()` function to fit the models with non-zero stacking coefficients to the full *training* set.


Please browse through the [getting started with `{stacks}`](https://stacks.tidymodels.org/articles/basics.html) vignette, and we'll implement a model stack at our next class meeting.

***

## Summary

In this notebook we were introduced the `{stacks}` package for building a custom ensemble of models. This expands our ability to create ensembles of models beyond just *gradient boosted trees* and *random forests* and allows us to make use of all valuable models that we've trained rather than being forced to decide on a *single* best model.