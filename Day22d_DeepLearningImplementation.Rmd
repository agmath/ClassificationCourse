---
title: 'Deep Learning: Implementation'
output:
  html_document:
    theme: cerulean
    highlight: pygments
  pdf_document: default
---

```{r global-options, include=FALSE}
#knitr::opts_chunk$set(eval = FALSE)
library(tidyverse)
library(tidymodels)
library(kableExtra)
library(tensorflow)
library(keras)
library(reticulate)

my_path_to_python <- "C:/users/agilb/AppData/Local/Programs/Python/Python310/python.exe"
```

**Purpose:** The previous two notebooks have discussed theoretical and foundational aspects of deep learning models. In particular, what types of architectures and activation functions exist (and, to a lesser extent, how do I choose one). In this notebook our goal will be to actually build, assess, and utilize a deep learning network for image classification.

## Setup

Follow the instructions to [install Keras and Tensorflow here](https://tensorflow.rstudio.com/install/). Note, the instructions seemed to be somewhat finicky. Here is what has worked for me.

1. Install TensorFlow with `install.packages("tensorflow")`
2. Run `library(reticulate)` -- install the `reticulate` package if you don't have it already. This will allow you to run Python and R together in a notebook and pass objects between environments.
3. Open your *terminal* using the *terminal* tab in the bottom-left pane of RStudio and type `python` and run the command by hitting `Enter`/`Return`.

  + If you get a Python interpreter -- indicated by `>>>` -- then you have python installed on your computer.
  
    + In this case, run `import sys` next to your python prompt.
    + Next run `sys.executable` to discover the path to your python executable.
    + Now, back in your R Console (click on the *Console* tab), define `path_to_python` as the path you've discovered. Don't forget to use quotes to surround the file path.
  
  + If you don't get the python prompt, then you'll need to install python.
  
    + Define `path_to_python <- install_python()` in your console and run it.

4. In your *Console*, run `virtualenv_create("r-reticulate", python = path_to_python)`. This will create a python virtual environment and package container in your working directory.
5. Install the `keras` package and then load the `keras` library.
6. Install TensorFlow, SciPy, and several TensorFlow data sets using `install_keras(method = "virtualenv", envname = "r-reticulate", version = "cpu")`.
7. Completely shut down and reopen RStudio. You might try clicking on *Session* and Restart R, but this only sometimes worked for me.
8. Reopen RStudio and your R Markdown notebook.
9. In the notebook, load the `tensorflow` and `reticulate` libraries.
10. Type and run `use_virtualenv("r-reticulate")`.
11. Run `tf$constant("Hello TensorFlow!")`

  + In addition to a notification, you should see `tf.Tensor(b'Hello TensorFlow!', shape = (), dtype = string)`.

## Data and Modeling

Now that TensorFlow is all set up, let's get some data. We'll use the Fashion MNIST dataset. You can learn more about that data set from [its official repository here](https://github.com/zalandoresearch/fashion-mnist).

```{r}
c(c(x_train, y_train), c(x_test, y_test)) %<-% keras::dataset_fashion_mnist()

x_train <- x_train/255
x_test <- x_test/255

labels_df <- tibble(label = seq(0, 9, 1),
                    item = c("Tshirt",
                             "Trousers",
                             "Pullover",
                             "Dress",
                             "Coat",
                             "Sandal",
                             "Shirt",
                             "Sneaker",
                             "Bag",
                             "AnkleBoot"))

rotate_img <- function(x){
  return(t(apply(x, 2, rev)))
}
```

In the code block above, we loaded the Fashion MNIST data, which comes already packaged into training and test sets. We then scaled the pixel densities from integer values (between 0 and 255) to floats. We created a data frame of labels for convenience, since the labels in `y_train` and `y_test` are numeric only. Finally, we wrote a function to rotate the matrix of pixel intensities so that the images will be arranged vertically when we plot them -- this is important for us humans but of no importance to the neural network we'll be training. 

Let's take a look at a few items and their labels.

```{r}
item_num <- 4
image(rotate_img(x_train[item_num, , ]))
labels_df %>%
  filter(label == y_train[item_num])
```

Okay -- I'm having a difficult time identifying these items. Can we train a sequential neural network to learn the classes?

```{r}
model <- keras_model_sequential(input_shape = c(28, 28)) %>%
  layer_flatten() %>%
  layer_dense(128, activation = "relu") %>%
  layer_dropout(0.2) %>%
  layer_dense(10)

model
```

We have a model with over 100,000 parameters! Because random weights are initially set for each of these, we can use the model straight "out of the box" for prediction. We shouldn't expect the network to perform very well though.

```{r}
predictions <- predict(model, x_train[1:2, , ])
#Predictions as a vector of log-odds
predictions
#Predictions as class-membership probabilities
tf$nn$softmax(predictions)
```

Let's define a loss function so that we can train the model by optimizing the loss.

```{r}
loss_fn <- loss_sparse_categorical_crossentropy(from_logits = TRUE)
loss_fn(y_train[1:2], predictions)
```

Before training, we'll need to set the optimizer, assign the loss function, and define the performance metric. We'll then compile the model with these attributes.

```{r}
model %>%
  compile(
    optimizer = "adam",
    loss = loss_fn,
    metrics = "accuracy"
  )
```

Note that, unlike most actions in R, the `model` object is updated here without explicitly overwriting the object. This is because the underlying process is being completed in the Python Environment and then the transformed object is being passed back to R via `reticulate`.

Since the model has been compiled, we are ready to train it. Again, we won't have to explicitly overwrite the model since the work is being done in Python and the objects passed back and forth.

```{r}
model %>% fit(x_train, 
              y_train, 
              epochs = 5)
```

Now let's evaluate our model performance.

```{r}
model %>%
  evaluate(x_test, y_test, verbose = 2)
```

We got 88% accuracy with a pretty vanilla and shallow neural network. There was only one hidden layer here, with 20% dropout. We didn't tune any model hyperparameters and only trained over 5 epochs. We can see that loss was continuing to decrease and accuracy was continuing to climb from one epoch to the next here.

Since our model has been trained, we can use it to make predictions again.

```{r}
predictions <- model %>%
  predict(x_test[1:5, , ])
tf$nn$softmax(predictions)
```

We can update our model so that it will provide class predictions rather than just the class membership probabilities.

```{r}
probability_model <- keras_model_sequential() %>%
  model() %>%
  layer_activation_softmax() %>%
  layer_lambda(tf$argmax)

probability_model %>%
  predict(x_test[1:5, , ])
```

Now that we've trained an assessed one neural network, go back and change your model. Add hidden layers to make it a true deep learning model. Experiment with the dropout rate or activation functions. Just remember that you'll need 10 neurons in your output layer since we have 10 classes and that the activation function used there should remain `softmax` since we are working on a multiclass classification problem. Everything else (other than the input shape) is fair game to change though!

## Summary

In this notebook we installed and used TensorFlow from R to build and assess a shallow learning network to classify clothing items from very pixelated images. The images were $28\times 28$. We saw that even a "simple" neural network was much better at predicting the class of an item based off of its pixelated image than we are as humans.