---
title: "Regression Versus Classification"
author: 
  - "Dr. Gilbert"
format: html
---

```{r setup, include = FALSE}
library(tidyverse)
library(tidymodels)
```

**Objectives:** In this notebook, our goals are to differentiate between *regression* and *classification*. In particular, we'll recognize the following:

+ Regression models are built to predict or explain a *numerical response*.
+ Classification models are build to predict or explain a *categorical response*.
+ Regression models seek to fit the training data as closely as possible -- your mental image might be as a "line of best fit".
+ Classification models seek to separate classes -- your mental image might be that classifiers seek to "draw fences".

## Toy Data for Classification

As we saw in MAT300, it is sometimes useful to have some "fake" data to play with in order to understand classification models. I've built several data sets below. I encourage you to play around by switching data sets and by making changes to the data sets I've provided.

```{r}
# #Easy two-class
# num_a <- 25
# num_b <- 30
# 
# set.seed(123)
# x_a <- rnorm(num_a, 20, 5)
# y_a <- rnorm(num_a, 20, 5)
# x_b <- rnorm(num_b, 50, 5)
# y_b <- rnorm(num_b, 50, 5)
# 
# toy_data <- tibble(x = c(x_a, x_b), y = c(y_a, y_b), class = c(rep("A", num_a), rep("B", num_b)))

#Second separable two-class with rare class
num_points <- 1000
x <- runif(num_points, 0, 50)
y <- runif(num_points, 0, 50)
class <- ifelse(sqrt((x - 20)^2 + (y - 40)^2) < 3,
                "A", "B")
toy_data <- tibble(x = x, y = y, class = class)

##Separable three-class
# num_a <- 10
# num_b <- 15
# num_c <- 12
# 
# x_a <- rnorm(num_a, 20, 5)
# y_a <- rnorm(num_a, 20, 3)
# x_b <- rnorm(num_b, 40, 3)
# y_b <- rnorm(num_b, 40, 3)
# x_c <- rnorm(num_c, 60, 3)
# y_c <- rnorm(num_c, 20, 3)
# 
# toy_data <- tibble(x = c(x_a, x_b, x_c), y = c(y_a, y_b, y_c), class = c(rep("A", num_a), rep("B", num_b), rep("C", num_c)))

# ##Two class rolled
# num_a <- 250
# num_b <- 175
# 
# x_a <- runif(num_a, 0, 40)
# x_b <- runif(num_b, 25, 60)
# y_a <- -(x_a - 25)^2 + 475 + rnorm(num_a, mean = 0, sd = 50)
# y_b <- (x_b - 40)^2 + rnorm(num_b, mean = 0, sd = 50)
# 
# toy_data <- tibble(x = c(x_a, x_b), y = c(y_a, y_b), class = c(rep("A", num_a), rep("B", num_b)))
```

```{r}
toy_data %>%
  ggplot() + 
  geom_point(aes(x = x, y = y, color = class))
```

Now that we have some toy data, we'll build several classifiers.

```{r}
dt_clf <- decision_tree() %>%
  set_engine("rpart") %>%
  set_mode("classification")

dt_rec <- recipe(class ~ x + y, data = toy_data)

dt_wf <- workflow() %>%
  add_model(dt_clf) %>%
  add_recipe(dt_rec)

dt_fit <- dt_wf %>%
  fit(toy_data)

new_data <- crossing(x = seq(min(toy_data$x) - 5, 
                             max(toy_data$x) + 5, 
                             length.out = 150),
                     y = seq(min(toy_data$y) - 5, 
                             max(toy_data$y) + 5, 
                             length.out = 150))

new_data <- dt_fit %>%
  augment(new_data)

ggplot() + 
  geom_point(data = new_data,
             aes(x = x, y = y, color = .pred_class),
             alpha = 0.05) +
  geom_point(data = toy_data,
             aes(x = x, y = y, color = class)) +
  labs(title = "Class Regions for Decision Tree",
       color = "Class")
```

Now a KNN Classifier...

```{r}
knn_clf <- nearest_neighbor(neighbors = 3) %>%
  set_engine("kknn") %>%
  set_mode("classification")

knn_rec <- recipe(class ~ x + y, data = toy_data)

knn_wf <- workflow() %>%
  add_model(knn_clf) %>%
  add_recipe(knn_rec)

knn_fit <- knn_wf %>%
  fit(toy_data)

new_data <- crossing(x = seq(min(toy_data$x) -5,
                             max(toy_data$x) + 5,
                             length.out = 150),
                     y = seq(min(toy_data$y) - 5,
                             max(toy_data$y) + 5,
                             length.out = 150))

new_data <- knn_fit %>%
  augment(new_data)

ggplot() + 
  geom_point(data = new_data,
             aes(x = x, y = y, color = .pred_class),
             alpha = 0.05) +
  geom_point(data = toy_data,
             aes(x = x, y = y, color = class)) +
  labs(title = "Class Regions for KNN (n = 3)",
       color = "Class")
```

## Assessing Classifier Performance

There are several performance measures for classifiers. Perhaps the most obvious measure is *accuracy* -- the proportion of observations correctly classified by our model. We'll encounter others as well -- accuracy is not always (or perhaps even often) the most appropriate performance measure for our classifiers.

Before getting into computing *accuracy* and our other performance metrics, it is worth talking about a *confusion matrix*. Confusion matrices summarize our data and predictions -- the true classes of the observations are along the rows and the predicted classes are along the columns. For example, a confusion matrix for our decision tree classifier from earlier appears below.

```{r}
dt_fit %>%
  augment(toy_data) %>%
  count(class, .pred_class) %>%
  pivot_wider(id_cols = class, 
              names_from = .pred_class, 
              values_from = n) %>%
  replace(is.na(.), 0)
```

### Reading the Confusion Matrix

### Computing Accuracy

### Why Not Accuracy?

### Precision

### Recall

### F1 Score